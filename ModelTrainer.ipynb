{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2023 The MediaPipe Authors. All Rights Reserved."
      ],
      "metadata": {
        "id": "QRXrUrT7vpJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "32O3JsNL7AxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "!wget -q -O photo.jpg https://storage.googleapis.com/mediapipe-demo/fist.jpg\n",
        "\n",
        "img = cv2.imread(\"photo.jpg\")\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "y8-DwtpLp7Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O rps.zip https://storage.googleapis.com/mediapipe-tasks/gesture_recognizer/rps_data_sample.zip\n",
        "!unzip -qq rps.zip"
      ],
      "metadata": {
        "id": "_0uT0VRAtE3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "NUM_EXAMPLES = 5\n",
        "IMAGES_PATH = \"rps_data_sample\"\n",
        "\n",
        "# Get the list of labels from the list of folder names.\n",
        "labels = []\n",
        "for i in os.listdir(IMAGES_PATH):\n",
        "  if os.path.isdir(os.path.join(IMAGES_PATH, i)):\n",
        "    labels.append(i)\n",
        "\n",
        "# Show the images.\n",
        "for label in labels:\n",
        "  label_dir = os.path.join(IMAGES_PATH, label)\n",
        "  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n",
        "  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n",
        "  for i in range(NUM_EXAMPLES):\n",
        "    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n",
        "    axs[i].get_xaxis().set_visible(False)\n",
        "    axs[i].get_yaxis().set_visible(False)\n",
        "  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kKCar6AhDuwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making a New Model\n",
        "\n"
      ],
      "metadata": {
        "id": "qmgcSMPeyk0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q mediapipe-model-maker"
      ],
      "metadata": {
        "id": "gxKdCdj2y_IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules.\n",
        "from mediapipe_model_maker import gesture_recognizer"
      ],
      "metadata": {
        "id": "GTCjgBETDqU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the rock-paper-scissor image archive.\n",
        "data = gesture_recognizer.Dataset.from_folder(\n",
        "    dirname=IMAGES_PATH,\n",
        "    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
        ")\n",
        "\n",
        "# Split the archive into training, validation and test dataset.\n",
        "train_data, rest_data = data.split(0.8)\n",
        "validation_data, test_data = rest_data.split(0.5)"
      ],
      "metadata": {
        "id": "Xh33Bg6QC5cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "hparams = gesture_recognizer.HParams(export_dir=\"rock_paper_scissors_model\")\n",
        "options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n",
        "model = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ],
      "metadata": {
        "id": "YE-CyQglHQD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(test_data, batch_size=1)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
      ],
      "metadata": {
        "id": "ULANbzL7HnIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the model bundle.\n",
        "model.export_model()\n",
        "\n",
        "# Rename the file to be more descriptive.\n",
        "!mv rock_paper_scissors_model/gesture_recognizer.task rock_paper_scissors.task"
      ],
      "metadata": {
        "id": "aMZQc767I9Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"rock_paper_scissors.task\")"
      ],
      "metadata": {
        "id": "pHxU6SjoLW-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "img = cv2.imread(\"photo.jpg\")\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "7fPaL2Uv07l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports neccessary modules.\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "# Create a GestureRecognizer object.\n",
        "model_path = os.path.abspath(\"rock_paper_scissors.task\")\n",
        "recognizer = vision.GestureRecognizer.create_from_model_path(model_path)\n",
        "\n",
        "# Load the input image.\n",
        "image = mp.Image.create_from_file('photo.jpg')\n",
        "\n",
        "# Run gesture recognition.\n",
        "recognition_result = recognizer.recognize(image)\n",
        "\n",
        "# Display the most likely gesture.\n",
        "top_gesture = recognition_result.gestures[0][0]\n",
        "print(f\"Gesture recognized: {top_gesture.category_name} ({top_gesture.score})\")"
      ],
      "metadata": {
        "id": "iiXfexYg0fph"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}